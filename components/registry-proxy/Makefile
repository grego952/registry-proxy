# Image URL to use all building/pushing image targets
RP_IMG ?= registry-proxy-controller:main
# ENVTEST_K8S_VERSION refers to the version of kubebuilder assets to be downloaded by envtest binary.
ENVTEST_K8S_VERSION = 1.31.0

PROJECT_ROOT=../..


# Get the currently used golang install path (in GOPATH/bin, unless GOBIN is set)
ifeq (,$(shell go env GOBIN))
GOBIN=$(shell go env GOPATH)/bin
else
GOBIN=$(shell go env GOBIN)
endif

include ${PROJECT_ROOT}/hack/tools.mk
include ${PROJECT_ROOT}/hack/help.mk

# CONTAINER_TOOL defines the container tool to be used for building images.
# Be aware that the target commands are only tested with Docker which is
# scaffolded by default. However, you might want to replace it to use other
# tools. (i.e. podman)
CONTAINER_TOOL ?= docker

# Setting SHELL to bash allows bash commands to be executed by recipes.
# Options are set to exit when a recipe line exits non-zero or a piped command fails.
SHELL = /usr/bin/env bash -o pipefail
.SHELLFLAGS = -ec

.PHONY: all
all: test

##@ Development

.PHONY: generate
generate: controller-gen ## Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations.
	$(CONTROLLER_GEN) object paths="./..."

.PHONY: manifests
manifests: controller-gen kubebuilder ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.
	rm -rf $(PROJECT_ROOT)/config/registry-proxy-autogenerated || true
	$(CONTROLLER_GEN) rbac:roleName=manager-role crd webhook \
		paths="$(PROJECT_ROOT)/components/registry-proxy/..." \
		output:rbac:artifacts:config=$(PROJECT_ROOT)/config/registry-proxy-autogenerated/rbac\
		output:crd:artifacts:config=$(PROJECT_ROOT)/config/registry-proxy-autogenerated/crd 
	# connectivityproxies exists only as a Go language mapping for the CRD
	# we don't want to install it on the cluster, as the conenctivity-proxy already provides this CRD
	rm $(PROJECT_ROOT)/config/registry-proxy-autogenerated/crd/operator.kyma-project.io_connectivityproxies.yaml || true
	# copy files over to the Helm chart, applying additional config

	# RBAC: change name & add labels
	sed \
		-e 's/name: manager-role/name: registry-proxy-manager-role/g' \
		-e 's/metadata:/metadata:\n  labels:\n    {{- include "chart.labels" . | nindent 4 }}/g' \
		$(PROJECT_ROOT)/config/registry-proxy-autogenerated/rbac/role.yaml \
		> $(PROJECT_ROOT)/config/registry-proxy/templates/rbac/role.yaml
	# CRD: add labels & annotation
	sed \
		-e 's/^metadata:/metadata:\n  labels:\n    {{- include "chart.labels" . | nindent 4 }}/g' \
		-e 's/annotations:/annotations:\n    "helm.sh\/resource-policy": keep/g' \
		$(PROJECT_ROOT)/config/registry-proxy-autogenerated/crd/registry-proxy.kyma-project.io_connections.yaml \
		> $(PROJECT_ROOT)/config/registry-proxy/templates/crd/registry-proxy.kyma-project.io_connections.yaml


.PHONY: test
test: generate fmt vet envtest ## Run tests.
	KUBEBUILDER_ASSETS="$(shell $(ENVTEST) use $(ENVTEST_K8S_VERSION) --bin-dir $(LOCALBIN) -p path)" go test $$(go list ./... | grep -v /e2e) -coverprofile cover.out

# TODO(user): To use a different vendor for e2e tests, modify the setup under 'tests/e2e'.
# The default setup assumes Kind is pre-installed and builds/loads the Manager Docker image locally.
# Prometheus and CertManager are installed by default; skip with:
# - PROMETHEUS_INSTALL_SKIP=true
# - CERT_MANAGER_INSTALL_SKIP=true
.PHONY: test-e2e
test-e2e: manifests generate fmt vet ## Run the e2e tests. Expected an isolated environment using Kind.
	@command -v kind >/dev/null 2>&1 || { \
		echo "Kind is not installed. Please install Kind manually."; \
		exit 1; \
	}
	@kind get clusters | grep -q 'kind' || { \
		echo "No Kind cluster is running. Please start a Kind cluster before running the e2e tests."; \
		exit 1; \
	}
	go test ./test/e2e/ -v -ginkgo.v

##@ Build

.PHONY: build
build: manifests generate fmt vet ## Build manager binary.
	go build -o bin/manager cmd/main.go

.PHONY: run
run: manifests generate fmt vet ## Run a controller from your host.
	go run ./cmd/main.go

# If you wish to build the manager image targeting other platforms you can use the --platform flag.
# (i.e. docker build --platform linux/arm64). However, you must enable docker buildKit for it.
# More info: https://docs.docker.com/develop/develop-images/build_enhancements/
.PHONY: docker-build
docker-build: ## Build docker image with the manager.
	$(CONTAINER_TOOL) build -t ${RP_IMG} -f Dockerfile ${PROJECT_ROOT}

.PHONY: docker-push
docker-push: ## Push docker image with the manager.
	$(CONTAINER_TOOL) push ${RP_IMG}

# PLATFORMS defines the target platforms for the manager image be built to provide support to multiple
# architectures. (i.e. make docker-buildx RP_IMG=myregistry/mypoperator:0.0.1). To use this option you need to:
# - be able to use docker buildx. More info: https://docs.docker.com/build/buildx/
# - have enabled BuildKit. More info: https://docs.docker.com/develop/develop-images/build_enhancements/
# - be able to push the image to your registry (i.e. if you do not set a valid value via RP_IMG=<myregistry/image:<tag>> then the export will fail)
# To adequately provide solutions that are compatible with multiple platforms, you should consider using this option.
PLATFORMS ?= linux/arm64,linux/amd64
.PHONY: docker-buildx
docker-buildx: ## Build and push docker image for the manager for cross-platform support
	# copy existing Dockerfile and insert --platform=${BUILDPLATFORM} into Dockerfile.cross, and preserve the original Dockerfile
	sed -e '1 s/\(^FROM\)/FROM --platform=\$$\{BUILDPLATFORM\}/; t' -e ' 1,// s//FROM --platform=\$$\{BUILDPLATFORM\}/' Dockerfile > Dockerfile.cross
	- $(CONTAINER_TOOL) buildx create --name registry-proxy-controller-builder
	$(CONTAINER_TOOL) buildx use registry-proxy-controller-builder
	- $(CONTAINER_TOOL) buildx build --push --platform=$(PLATFORMS) --tag ${RP_IMG} -f Dockerfile.cross ${PROJECT_ROOT}
	- $(CONTAINER_TOOL) buildx rm registry-proxy-controller-builder
	rm Dockerfile.cross

.PHONY: check-connectivityproxy-installation
check-connectivity-proxy-installation: ## Wait for Connectivity proxy CR to be in Ready state.
	# wait some time to make sure operator starts the reconciliation first
	sleep 10

	./hack/verify_connectivityproxy_status.sh || \
		(make print-connectivityproxy-details && false)

	kubectl wait --for condition=Available -n kyma-system deployment connectivity-proxy-operator --timeout=60s || \
		(make print-connectivityproxy-details && false)

.PHONY: print-connectivityproxy-details
print-connectivity-proxy-details: ## Print all pods, deploys and Connectivity proxy CRs in the kyma-system namespace.
	kubectl get connectivityproxy -n kyma-system -oyaml
	kubectl get deploy -n kyma-system -oyaml
	kubectl get pods -n kyma-system -oyaml

